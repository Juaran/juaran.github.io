---
title: 【研二上】图神经网络知识梳理
date: 2021-09-14 17:30:00
category: 机器学习
tag: 周报
---

## 传统机器学习

关键在于手动设计有效的图特征。泛化能力差，不能对新的节点预测。

### Node level

分为：基于重要性、基于结构的特征提取

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210911201034331.png" alt="image-20210911201034331" style="zoom:50%;" />

基于节点重要性：节点的度、节点中心性度量，用于获取图中较大影响力的节点

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210911201232400.png" alt="image-20210911201232400" style="zoom:50%;" />

基于节点结构：度、聚类系数、非同构子图集，用于获取图中特别部分（局部）

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210911201725541.png" alt="image-20210911201725541" style="zoom:50%;" />

### Link level

目标是基于已有的连接（边）预测可能存在的（新的）边。关键在于捕获**节点对**的特征。方法有：

* 基于节点距离：具有相同距离的节点间可能存在边
* 局部和全局邻节点重叠：共享多个邻节点的节点间可能存在边

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210911203159869.png" alt="image-20210911203159869" style="zoom:50%;" />

### Graph level

对整个图进行预测。采用设计**核**的方法而不是提取特征向量。

这些kenel的用来衡量两个图之间的相似性，从而做出预测。关键在于设计高效的核。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210911210607576.png" alt="image-20210911210607576" style="zoom:50%;" />

### Summarize

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210911210952206.png" alt="image-20210911210952206" style="zoom:50%;" />

## 表示学习方法

不再需要特征工程，自动学习特征。**目标**：进行高效的、与任务无关的图特征学习。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210911211659225.png" alt="image-20210911211659225" style="zoom:50%;" />

### 节点嵌入

主要任务是自动学习图特征并映射（编码）到d维空间的向量表示。这个特征表示也被称为**嵌入**，这些特征向量用于下游预测任务，如：节点分类、链路预测、图分类、匿名节点检测等。

通常对节点进行嵌入，即Node Embedding。启发是：图中相似的节点在嵌入空间中的位置也靠近。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210911212314632.png" alt="image-20210911212314632" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912144000584.png" alt="image-20210912144000584" style="zoom: 80%;" />

#### Encoder+Decoder Framework

将图节点编码到向量空间，并保持节点的相似性。编码前后节点的相似性表示：

* 图空间中两个节点相似度函数`similarity(u,v)`**需要被定义**
* 一般使用向量空间中的**点积**`ZuT·Zv`

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210911213916779.png" alt="image-20210911213916779" style="zoom:50%;" />

以下是节点嵌入的编码和解码过程：

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210911215015180.png" alt="image-20210911215015180" style="zoom:50%;" />

优化相似度函数similarity以近似于向量点积。因此可以将Node Embbedding视为浅层编码（Shallow Encoding）网络。缺点是图节点很多时编码效率低，参数量大、泛化能力差。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912141828869.png" alt="image-20210912141828869" style="zoom: 50%;" />

#### Random Walk

以上编解码框架中，关键在于如何定义图节点的相似性。

采用基于随机的方法对两个节点的相似性进行定义。待补充相关知识......

#### Summarize

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210911223007280.png" alt="image-20210911223007280" style="zoom:50%;" />

### 集体分类

根据邻节点标签、特征预测未知节点标签，本质工作是进行消息传递。启发是：节点间的**相关性**就存在于图结构中，即相似的节点总是相互连接。从社会科学的角度来讲：

* **同质性**Homophily：相似特征的一类人趋向于聚集在一起（集体），例如：社交网络、论文引用网络
* 影响力Influence：集体也会影响个体特征，例如：电商推荐系统

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912104601445.png" alt="image-20210912104601445" style="zoom:50%;" />

重要概念：**集体分类**，为图中所有节点分配标签。基于一阶马尔科夫假设，当前节点标签仅取决于邻节点标签。

主要过程是：初始化标签，捕获邻节点相关性，传播相关性推断标签。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912114845916.png" alt="image-20210912114845916" style="zoom:50%;" />

#### Relational Classifier

基本思想：未知节点所属标签概率是其邻节点的加权平均概率。缺点：没有利用邻节点的特征。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912144104092.png" alt="image-20210912144104092" style="zoom:80%;" />

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912144122895.png" alt="image-20210912144122895" style="zoom:80%;" />

#### Iterative Classification

主要思想：基于自身节点特征和邻节点标签做出预测。

#### Belief Propagation

置信传播：信息message在邻节点之间传递收集，更新自身信息并发送到下一节点。

缺陷：如果图中存在闭环，消息会重复传播。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912134707725.png" alt="image-20210912134707725" style="zoom:50%;" />

## GNN

本质是由浅层编码发展为多层非线性变换深度编码器，这些编码器用于自动学习节点相似性、收集并传递消息等。是前面两种表征学习方法的结合。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912144144627.png" alt="image-20210912144144627" style="zoom:80%;" />

### GCN对比CNN

网络基本结构：

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912144201665.png" alt="image-20210912144201665" style="zoom:80%;" />

图的基本内容：

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912144821468.png" alt="image-20210912144821468" style="zoom:50%;" />

**数据输入**：将图的邻接矩和节点特征组成增广矩阵，矩阵的每一行作为网络的输入。

缺点是参数量太大；不适用于不同大小的图；对节点的次序敏感。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912145106851.png" alt="image-20210912145106851" style="zoom:50%;" />

泛化图的卷积操作：转换节点的位置、标签、特征向量信息成一条message，汇总到下一节点。主要思想是转换、聚合、传播消息。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912150239541.png" alt="image-20210912150239541" style="zoom:50%;" />

模型需要学习的是不同节点的消息**转换函数**、消息**聚合函数**。

* Transform：将节点标签、特征压缩为一条消息
* Aggregation：收集邻节点的消息

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912151228887.png" alt="image-20210912151228887" style="zoom:50%;" />

与传统卷积网络非常大的不同之处是：**因为每个节点的邻节点情况都不同， 因此学习内容将是每个节点自身的网络模型表达！**具有相似结构的节点则可能学习到相似的模型。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912151942390.png" alt="image-20210912151942390" style="zoom:50%;" />

网络的层数可以是任意的，K-层嵌入则代表节点可以获取到距离K-跳的节点的信息。

基本方式：平均节点信息，并施加激活函数。初始层为节点特征向量。W、B权重和偏置矩阵，待优化参数。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912160323415.png" alt="image-20210912160323415" style="zoom:50%;" />

网络归纳能力（泛化能力）：所有相同结构的节点共享参数矩阵。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912162718153.png" alt="image-20210912162718153" style="zoom:50%;" />

### GNN结构

* **节点消息转换和聚合方式**的不同是不同架构的GNN网络的主要差异所在。

* 网络层数不是越多越好，层数堆叠意味着节点收集的**信息相互重叠**，同质化严重，最终得到相同的嵌入。

* 特征增强和结构增强是图增强的主要手段，图增强的目的是便于计算=>**输入图需要转为计算图**。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210911190706084.png" alt="image-20210911190706084" style="zoom:50%;" />

#### Single layer

每一层网络都包含两个部分：消息转换和消息聚合。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912171739617.png" alt="image-20210912171739617" style="zoom:50%;" />

**消息函数**：提取节点特征向量

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912172622859.png" alt="image-20210912172622859" style="zoom:50%;" />

**聚合函数**：对邻节点消息进行求和、平均或取最大值。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912172730288.png" alt="image-20210912172730288" style="zoom:50%;" />

激活函数：

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912175628479.png" alt="image-20210912175628479" style="zoom:50%;" />

**经典GCN**公式：

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912180300177.png" alt="image-20210912180300177" style="zoom:50%;" />

**GraphSAGE**在GCN基础上聚合了节点自身消息，并使用了多种聚合函数进行消息聚合和L2正则化：

* Mean 加权平均
* Pool 对多个层进行平均或最大池化
* LSTM

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912175400251.png" alt="image-20210912175400251" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912180432617.png" alt="image-20210912180432617" style="zoom:50%;" />

**GAT**注意力网络：之前的网络中每一个节点权重相同，GAT中将简单的度均分权重替换为**注意力权重**。

启发是：并不是所有到节点对导向节点的影响力都相同，应该着重关注影响力大的少部分节点、忽略其余大部分。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912194523822.png" alt="image-20210912194523822" style="zoom:50%;" />

**注意力机制**：计算两个节点对的注意力系数，然后归一化所有邻节点得到注意力权重

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912195329751.png" alt="image-20210912195329751" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912195520238.png" alt="image-20210912195520238" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912200315574.png" alt="image-20210912200315574" style="zoom:50%;" />

在实践中，很多深度学习的方法可以应用到GNN的layer中，如批归一化、Dropout、注意力、激活函数等。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912200736046.png" alt="image-20210912200736046" style="zoom:50%;" />

#### Stack layer

标准的做法：线性地叠加GNN层。

不同于CNN网络层，不是越深越好。多层GNN存在的问题：**过度平滑**，差异小，所有节点嵌入到同一个值。

原因是：随着层数的加深，不同节点聚集的信息几乎都来自图中相同的节点（高度重叠）。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912205135963.png" alt="image-20210912205135963" style="zoom:50%;" />

谨慎设置GNN层数L：盲目增加层数并不有效

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912210515784.png" alt="image-20210912210515784" style="zoom:50%;" />

层数不深网络**表达能力不强**的补救措施：

1. 增强每一层的信息聚集、信息转换，让转换、聚集部分成为深度网络
2. 增加预处理层和后处理层

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912210937776.png" alt="image-20210912210937776" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912211408933.png" alt="image-20210912211408933" style="zoom:50%;" />

层数不能太深的第二个做法：增加**跳过连接**，在早期选择性的跳过一些层。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912211743305.png" alt="image-20210912211743305" style="zoom:50%;" />

实际上增加短路连接使得网络获得了**不同层网络的混合**，使得表达能力提升。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912212215796.png" alt="image-20210912212215796" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210912212739801.png" alt="image-20210912212739801" style="zoom:50%;" />

### 图增强

原因：原始图可能缺乏特征，图结构过于稀疏或密集、或太大，不能直接用于网络输入（计算代价高）。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210913205207633.png" alt="image-20210913205207633" style="zoom:50%;" />

增强方法：

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210913205411953.png" alt="image-20210913205411953" style="zoom:50%;" />

为节点赋值常量或One-hot编码（增强节点特征）：

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210913205831216.png" alt="image-20210913205831216" style="zoom:50%;" />

对稀疏图增加虚拟节点和虚拟边；对密集图和大图进行采样以减少计算量：

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210913210511512.png" alt="image-20210913210511512" style="zoom:50%;" />

### 图训练

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210913211038818.png" alt="image-20210913211038818" style="zoom:50%;" />

* Prediction head：区分为节点级别 or 链路级别 or 图级别的训练任务

==待补充图训练pipeline==

### 表达能力

GNN的本质是通过多层非线性感知器聚合局部邻节点信息来生成当前节点的嵌入。经典网络：

* GCN 采用平均池化（mean-pool）的方式聚合信息：邻居具有相同权重
* GraphSAGE 采用最大池化（max-pool）的方式聚合信息：采样大部分邻居

以上两种聚合方式都存在缺陷，将导致某些情况下模型的表达能力欠缺。以下进行GNN理论分析：

假设在以下情况中，讨论GNN是否能**区分不同子节点的局部图结构**：

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210914110819051.png" alt="image-20210914110819051" style="zoom:50%;" />

根据GNN的聚合信息的思想，引入**计算图**（Computational Graph）的概念，每个节点的最终嵌入是由不同层的邻节点的信息聚合计算得到。显然，节点1和2的计算图是相同的，GNN最终无法区分：

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210914112033591.png" alt="image-20210914112033591" style="zoom:50%;" />

因此，GNN学习的是不同**节点子树结构**的表达。对于相同子树结构的节点，训练得到相同参数的模型；不同子树结构节点的网络模型汇总了GNN的整体表达能力。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210914112945018.png" alt="image-20210914112945018" style="zoom:50%;" />

所以，多层感知的聚合函数能够增强GNN表达能力；而将所有类型的子树结构映射到嵌入空间中，将发挥GNN的最大表达能力。还需要一个**单射函数**用于区分不同的子树结构，能够最大程度地保留所有节点信息。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210914120635491.png" alt="image-20210914120635491" style="zoom:50%;" />

GCN采用平均池化聚合信息时的丢失邻节点数量信息：

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210914163725193.png" alt="image-20210914163725193" style="zoom:50%;" />

GraphSAGE采用最大池化聚合信息时丢失比例信息：

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210914164035845.png" alt="image-20210914164035845" style="zoom:50%;" />

以上两种网络对于不同结构的子树得到了相同的嵌入（非单映射），必然导致网络的表达能力下降。基于此理论分析产生了“最具有表达能力”的同构图神经网络GIN。

> 论文：HOW POWERFUL ARE GRAPH NEURAL NETWORKS?
> 作者：Keyulu Xu,Weihua Hu, Jure Leskovec
> 来源：ICLR 2019