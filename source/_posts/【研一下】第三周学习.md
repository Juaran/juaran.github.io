---
 
title: 【研一下】第三周学习汇报
date: 2021-3-15
category: 机器学习
tag: 周报
 
---

# Day 01

对机器学习的初步理解。机器学习分为监督学习，无监督学习，强化学习。监督学习的典型问题有：回归、分类，无监督学习的问题主要有聚类，强化学习例如Google的围棋，自己与自己下棋获得学习能力。

### Step 1 Model模型：函数集合function set

以”预估房价“这一典型回归问题为例，机器学习的过程可以看做：Y = f(X) 求解最佳 f 的过程，在训练过程中，X为房子户型，户型有多个特征如：面积、高度......，Y为已知历史房价；在无限个可能的function当中，找出最适合的、能最大程度拟合房价分布的曲线，这些所有可能的函数集合也被称为**假设空间**。

### Step 2 策略：Loss损失函数

由于假设空间太大，需要选择其中的“最优解”，Loss函数的作用就是为了寻找与输入数据相吻合的一条曲线f。损失函数是评估假设空间中函数“好坏”的函数，损失率越大，该函数拟合效果越差，对训练样本的拟合程度越低，训练误差大，称为**欠拟合**；损失率越低，对训练集拟合程度越高，但可能对测试数据预测偏差大，测试误差大，称为**过拟合**。

为了平衡欠拟合与过拟合的情况，采用**正则化**的方式降低参数对输出的影响程度。

### Step 3 算法：Gradient Descent梯度下降

梯度下降是为了找到最优的Loss函数，即找到损失最小时的假设空间函数f。梯度下降可以简单看做更新微积分项的过程，在曲线“陡峭”的地方，“下降”的步伐越大，曲线“平缓”的地方，“下降”的步伐越小，逐步接近极小值。**学习速率**是为了控制下降速度的一个参数。

# Day 02

对于分类问题的机器学习方法，通常采用**逻辑回归**的做法。线性回归用于预测离散数值的大小，其假设空间函数是线性的；逻辑回归用于分类，如人脸识别、手写字体检测、声音识别等，其假设空间函数通常是离散的概率分布函数，比如识别猫还是狗，利用**贝叶斯公式**计算出在某种情况下该类别发生的可能性。**生成模型**Generation Model即可以根据已知输入得到的贝叶斯概率，生成未知离散点的概率，从而预测该事件发生的概率。

神经网络Neural Network的假设空间可以看做是Neural Structure神经结构，每一层有多个Neuron神经元组成，每个Neuron由初始的输入层X和权重Weight、偏差Bias矩阵运算得到，经过中间多个隐藏层全连接Full connection layer得到输出层Y。机器学习第二步是判断神经结构的“好坏”，这需要调整每个神经元的参数W和b，是参数优化的过程。

卷积神经网络Converlutional Neural Network在神经网络的基础上，减少了很多的“连接”，不再是Full Connection，做法是将输入层内积多个不同的Filter，得到新的神经元，进行Max Pooling池化，即选出卷积后的最大值，不断重复内积和池化的过程，将输出结果在再作为全连接层神经网络的输入。以图像输入为例，每次卷积得到的层会使得不同纹理的区域分开，但又共享各个纹理的参数，所以下次卷积得到两种纹理叠加的形态，最终输出更加完整的图像。

# Day 03

回归实验：使用回归方法预测大学城附近二手房售价。

## 一、数据准备

### 1.1 二手房信息

数据来源：安居客福州市闽侯区大学城二手房信息。

主要抓取**面积-售价**数据，在作回归分析时将面积作为输入X，房售价为输出Y。

剔除其中面积高于500平的数据。一共获取到**2577**条数据，面积单位平方米，价格单位万元。

![image-20210318165424127](https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210318165424127.png)![image-20210318165450452](https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210318165450452.png)

``` python
import requests
from lxml import etree
import csv
import time
 
 
def get_page_data(page):
    print("[*] Getting page", page)
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36"
    }
    # 链接来源：安居客福州市闽侯区大学城二手房信息
    url = "https://fz.anjuke.com/sale/minhou-q-dxch/o2-p{}/".format(page)
    res = requests.get(url, headers=headers)
 
    # 解析二手房面积和价格
    tree = etree.HTML(res.text)
    areas = tree.xpath('//*[@id="__layout"]/div/section/section[3]/section[1]/section[2]/div/a/div[2]/div[1]/section/div[1]/p[2]/text()')
    areas = [area.strip().rstrip('㎡') for area in areas]
    prices = tree.xpath('//*[@id="__layout"]/div/section/section[3]/section[1]/section[2]/div/a/div[2]/div[2]/p[1]/span[1]/text()')
 
    if len(prices) == 0:
        print("[!] Data not found! ")
 
    samples = list(zip(areas, prices))
 
    header = ['area', 'price']
    with open('data.csv', mode='a', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerows(samples)
 
    time.sleep(10)
 
 
# 创建 csv 数据文件并添加表头
header = ['area', 'price']
with open('data.csv', mode='w', encoding='utf-8', newline='')    as f:
    writer = csv.writer(f)
    writer.writerow(header)
 
for page in range(1, 46):  # 遍历全部页面，手动翻页到46结束
    get_page_data(page)
```

### 1.2 数据分类

随机抽取577条数据作为测试数据**test.csv**，剩余2000条作为训练数据**train.csv**：

![image-20210318170019936](https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210318170019936.png)

``` python
import csv
import random
 
header = ['area', 'price']
f_read = open('data.csv', 'r')
f_test = open('test.csv', 'w', newline='')
f_train = open('train.csv', 'w', newline='')
 
csv_read =  csv.reader(f_read)
csv_test = csv.writer(f_test)
csv_test.writerow(header)
csv_train = csv.writer(f_train)
csv_train.writerow(header)
 
 
# 随机抽取不重复577条数据
random_index = random.sample(range(2577), 577)
for i, row in enumerate(csv_read):
    if i in random_index:
        csv_test.writerow(row)  # 写入测试数据
    else:
        csv_train.writerow(row)  # 写入训练数据
 
f_test.close()
f_train.close()
f_read.close()
```

### 1.3 可视化

![](https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210318170914561.png)

![](https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210318170854066.png)

``` python
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.pyplot import MultipleLocator
 
# 读取训练数据
train_data = pd.read_csv('train.csv')[1:][::-1]
# Series转为list类型
x = train_data['area'].tolist()  # X轴：area
y = train_data['price'].tolist()  # Y轴：price
 
plt.scatter(x, y, color='dodgerblue')
plt.show()
```

``` python
# 读取测试数据
test_data = pd.read_csv('test.csv')[1:][::-1]
# Series转为list类型
x_test = test_data['area'].tolist()  # X轴：area
y_test = test_data['price'].tolist()  # Y轴：price
 
plt.scatter(x_test, y_test, color='aquamarine')
plt.show()
```

## 二、回归

### 2.1 模型一

第一步，选择模型。首先选用最简单的**线性模型**：
$$
f(x) = y = w·x + b
$$
y代表售价，x代表面积，w和b为待求参数，可以看做权重和偏差。根据不同w和b组成的所有函数集合构成模型的**假设空间**。

第二步，选择策略。定义**损失函数**Loss用于评估一个模型函数f对样本点的拟合程度：
$$
L(f) = L(w, b) = \sum_{i=1}^n (y^i - (w · x^i + b))^2
$$
其中y(i)代表第i个样本点的售价，x(i)代表第i个样本点面积。是最常见的平方损失函数。如果L(f)越大，说明该f对所有样本的拟合程度越低，反之L(f)越小，拟合程度越高。

第三步，梯度下降算法。由于假设空间无限大，需要利用梯度下降算法找到损失最小的函数。原理是分别对损失函数中的w和b作偏微分，并以此更新下一次w和b的值，迭代数次之后将得到最优解。
$$
w^1 = w^0 - \eta \frac{dL}{dw} |_{w = w^0}
$$

$$
b^1 = b^0 - \eta \frac{dL}{dw} |_{b = b^0}
$$

其中eta为学习速率，也即梯度下降的速率，体现在参数增加或减少的大小。

 

最终计算得到的最优函数为：
$$
y = w·x + y , w = 2.19, b = -0.05
$$
训练和测试结果：

![image-20210318180053672](https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210318180053672.png)

![image-20210318180236140](https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210318180236140.png)

``` python
# 由损失函数计算error值
def get_loss_model_1(w, b, points):
    total_error = 0
    for i in range(len(points)):
        xi, yi = float(points[i][0]), float(points[i][1])
        total_error = total_error + (yi - (w * xi + b)) ** 2
 
    return total_error / float(len(points))
 
 
# compute gradient
def step_gradient(b_current, w_current, points, learningRate):
    b_gradient, w_gradient = 0, 0
    for i in range(len(points)):
        xi,yi = float(points[i][0]), float(points[i][1])
        b_gradient += 2 * ((w_current * xi) + b_current - yi)
        w_gradient += 2 * xi * ((w_current * xi) + b_current - yi)
    b_gradient = b_gradient / float(len(points))
    w_gradient = w_gradient / float(len(points))
    new_b = b_current - (learningRate * b_gradient)
    new_w = w_current - (learningRate * w_gradient)
#     print(new_b, new_w)
    return new_b, new_w
 
def gradient_descent_runner(points, starting_b, starting_w, learning_rate, num_iterations): # num_iteration 迭代次数
    b, w = starting_b, starting_w
    for i in range(num_iterations):
        b, w = step_gradient(b, w, points, learning_rate)
#         print("Loss", get_loss_model_1(w, b, points))
    return w, b
 
learning_rate = 0.00001  # 学习速率
initial_w = 2.0  # weight
initial_b = 0  # bias
num_iterations = 100  # 梯度下降次数
points = list(zip(x, y))  # 点集
 
print(f"Starting gradient descent at b = {initial_b}, w = {initial_w}, error = {get_loss_model_1(initial_w, initial_b, points)}")
w, b = gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations)
print(f"After {num_iterations} iterations at b = {b}, w = {w}, error = {get_loss_model_1(w, b, points)}")
```

> ```
> Starting gradient descent at b = 0, w = 2.0, error = 13242.886893446723
> After 100 iterations at b = -0.05052018426808582, w = 2.1934384452528293, error = 12503.517684126853
> ```

``` python
# 显示训练数据和拟合曲线
plt.scatter(x, y, color='dodgerblue')
x_line = x
y_line = [w * x + b for x in x_line]
plt.plot(x_line, y_line, color='red')
plt.show()
```

``` python
# 显示测试数据和拟合曲线
 
test_points = list(zip(x_test, y_test))  # 点集
print(f"Test loss at b = {b}, w = {w}, error = {get_loss_model_1(w, b, test_points)}")
 
plt.scatter(x_test, y_test, color='aquamarine')
 
x_major_locator = MultipleLocator(100)
y_major_locator = MultipleLocator(200)
ax = plt.gca()
ax.xaxis.set_major_locator(x_major_locator)
ax.yaxis.set_major_locator(y_major_locator)
 
x_line = x_test
y_line = [w * x + b for x in x_line]
plt.plot(x_line, y_line, 'r')
 
plt.show()
```

### 2.2 模型二

模型一欠拟合，训练误差和测试误差都很大。考虑增加x平方项的函数空间：
$$
f(x) = y = w_1·x + w_2·x^2 + b
$$
损失函数L：
$$
L(f) = L(w_1, w_2, b) = \sum_{i=1}^n (y^i - (w_1 · x^i + w_2 · (x^i)^2 + b))^2
$$
梯度下降时计算微分项稍有不同。

 

最终计算得到最优函数：
$$
y= w_1·x + w_2 · x^2 + y , w_1 = 2.35, w_2 = 0.006, b = 97
$$
训练和测试结果：

![](https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210318193550265.png)

![image-20210318193550265](https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210318194348273.png)

### 2.3 模型三

考虑增加x三次方项的函数空间：
$$
f(x) = y = w_1·x + w_2·x^2 + w_3 · x^3 + b
$$
损失函数L：
$$
L(f) = L(w_1, w_2, w_3, b) = \sum_{i=1}^n (y^i - (w_1 · x^i + w_2 · (x^i)^2 + w_3 · (x^i)^3 + b))^2
$$
梯度下降时计算微分项稍有不同。

 

训练和测试结果：

![image-20210318194348273](https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210318193642373.png)

![image-20210318194432245](https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210318194432245.png)

参考：

https://mathor.blog.csdn.net/article/details/103698507 用Numpy实现线性回归

## 三、结论

综上，模型二对该样本空间的拟合较为准确，训练误差和测试误差都较低，可以作为依据面积预测二手房售价问题的预测模型。

# Day 04

Scikit-learn(sklearn)是机器学习中常用的第三方模块，对常用的机器学习方法进行了封装，包括回归(Regression)、降维(Dimensionality Reduction)、分类(Classfication)、聚类(Clustering)等方法。

利用sklearn模块内置的回归方法对昨天的数据做回归分析，模型如下：

* Linear Regression（线性回归）
* Decision Tree Regressor（决策树回归）
* SVM Regressor（支持向量机回归）
* K Neighbors Regressor（K近邻回归）
* Random Forest Regressor（随机森林回归）
* Adaboost Regressor（Adaboost 回归）
* Gradient Boosting Random Forest Regressor（梯度增强随机森林回归）
* bagging Regressor（bagging 回归）
* ExtraTree Regressor（ExtraTree 回归）

参考：

https://blog.csdn.net/ChenVast/article/details/82107490 一些常用的回归模型实战（9种回归模型）

https://blog.csdn.net/weixin_42731853/article/details/110917315 机器学习入门之7种经典回归模型

# Day 05

学习numpy、pandas、matplotlib库的基本用法。