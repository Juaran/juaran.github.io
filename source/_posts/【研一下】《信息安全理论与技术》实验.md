---
 
title: 《信息安全理论与技术》实验
date: 2021-4-25
category: 研一下周报
 
---

 

## CNN分层特征可视化

**姓名**：xxx

**学号**：200320105

**指导教师**：钟尚平

<!-- more -->

## 一、实验目标

1. 使用CNN（卷积神经网络）训练图片数据完成机器学习分类任务
2. 在已训练好的模型上，可视化每一层网络节点提取的低级特征、中级特征和高级特征

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/1457797187_33943.png" alt="1457797187_33943"  />

## 二、实验内容

### 1. 数据集

实验将分别对两个数据集进行。一个是机器学习经典分类数据集CIFAR-10，包含60000张32x32大小的彩色图像，分为10个类别；另一个是自收集的明星人脸数据集，包含6719张224x224大小的彩色图像，也是10个类别。

* CIFAR-10数据集

该数据集可以从Pytorch的torchvision工具库直接引入，第一次引入时将从网络下载。训练数据集50000张，测试数据集10000张。数据集特点是图像不大，数据量充足，训练时模型参数少，可以达到很好的训练和测试效果。

``` python
from torchvision import datasets
 
train_dataset = datasets.CIFAR10(root='cifar10',  # 数据集根目录
                     transform=transforms.ToTensor(),  # 转换为Tensor
                     train=True)  # 训练数据集
```

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210425115908156.png" alt="image-20210425115908156"  />

* 明星人脸数据集

从网络中下载十位明星的图片，通过人脸剪裁工具检测并保存大小为224x224的人脸部分，训练数据集大小6719，每个人物平均约600张。数据集特点是图像大，数据量少，训练参数量多，人脸的区分度不如上一个数据集，因此训练较为困难。

``` python
import os
from PIL import Image
from torchvision import transforms
from torch.utils.data import Dataset
 
# 重写Dataset类
class MyDataset(Dataset):
    def __init__(self, data_dir, transform: transforms = None):
        self.data_info = self.get_img_info(data_dir)
        self.transform = transform
 
    def __getitem__(self, index):
        label, img_path = self.data_info[index]
        im = Image.open(img_path).convert('RGB')  # PIL.Image
        if self.transform is not None:
            im = self.transform(im)  # 图像变换
        return im, label   # (torch.Size([3, 224, 224]), 0)
 
    def __len__(self):
        return len(self.data_info)
 
    @staticmethod
    def get_img_info(data_dir):
        img_info = []
        for root, dirs, file in os.walk(data_dir):
            for sub_dir in dirs:  # 遍历类别目录
                img_label = int(sub_dir.split('-')[0])  # 提取类别标签
                img_names = os.listdir(os.path.join(root, sub_dir))  # 拼接图片路径
                img_paths = [os.path.join(root, sub_dir, img_name) for img_name in img_names]
                img_info.extend([(img_label, img_path) for img_path in img_paths])
 
        return img_info  # [(0, 'train_dataset/1-蔡徐坤/0002.jpg'), ...]
```

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210425120446353.png" alt="image-20210425120446353" style="zoom:80%;" />

### 2. 模型

本实验选择VGG-16卷积神经网络作为机器学习模型，其网络结构如下图所示。模型隐层包含前13个卷积层和后3个全连接层，每一层经过卷积或全连接+ReLU激活操作，中间有五个池化层。

![一文读懂VGG网络](https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/v2-dfe4eaaa4450e2b58b38c5fe82f918c0_1440w.jpg)

* 构建模型

``` python
from torch import nn
 
# 构建网络模型
class VGG16(nn.Module):
    def __init__(self):
        super(VGG16, self).__init__()
        self.net = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Flatten(),    
            nn.Linear(in_features=512, out_features=100),
            nn.ReLU(),
            nn.Linear(in_features=100, out_features=100),
            nn.ReLU(),
            nn.Linear(in_features=100, out_features=10),
            nn.ReLU(),
        )
 
    def forward(self, x):
        return self.net(x)
```

注意最后三个全连接层参数需要根据输入图像大小进行调整。按照VGG网络模型，当输入图像大小为224x224x3时，第一个全连接层输入为512x7x7，输出为4096（FC-4096、FC-4096、FC-10）；在CIFAR-10数据集上，输入图像32x32x3，得到第一个全连接层输入为512x1x1，相应的将全连接层输出调整为100（FC-100、FC-100、FC-10）。

* 模型参数

``` python
from torchsummary import summary
 
model = VGG16()
model.cuda()  
# summary(model, input_size=(3, 32, 32))
summary(model, input_size=(3, 224, 224))
```

> Input Size 3x32x32: Total params: 14,777,098
>
> Input Size 3x224x224: Total params: 134,301,514

### 3. 训练

* 开始训练

``` python
import torch
from torch import optim
from torch.utils.data import DataLoader
 
from utils import MyDataset
from vgg16 import VGG16
 
# 0. 读取显卡
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
print(torch.cuda.is_available())
# 1. 读取数据集
train_dataset = MyDataset(data_dir='train_dataset',
                          transform=transforms.Compose(
                              [transforms.Resize([224, 224]),
                               transforms.ToTensor()]
                          ))
# 2. 数据迭代器
train_batch_size = 32
train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)
# 3. 实例化模型
model = VGG16()
model.cuda()
# 如果保存了模型参数，加载最后一次参数继续训练
# model.load_state_dict(torch.load(f='models/state_dict_epoch100_50.pth'))
# 4. 训练设置
epochs = 50  # 训练轮数
display = 100  # 显示步长
train_batch_size = 32  # 训练批次大小
# 数据加载器（迭代器，每次迭代返回一个batch大小的数据data+标签target）
train_dataloader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True)
# 优化器、损失函数
optimizer = optim.Adam(params=model.parameters(), lr=1e-3)
criteon = torch.nn.CrossEntropyLoss()  # 交叉熵损失
# 5. 开启训练
model.train()
for epoch in range(epochs):
    for i, train_data in enumerate(train_dataloader):
        # 正向传播
        data, target = train_data  # 获取数据和标签
        data = data.to(device)    # 数据载入显卡
        target = target.to(device)    # 数据载入显卡
        pred = model(data)  # 模型输出预测值
        loss = criteon(pred, target)  # 损失
        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        # 显示Loss
        if i % display == 0:
            print('Epoch', epoch + 1, (i + 1) * train_batch_size, '/ 50000', 'Loss', loss.item())
            # 6. 每10轮保存一次模型
            if epoch % 10 == 0: 
                model_epoch = 'models/cifar10_epoch_' + str(epoch+10) + '.pth'
                torch.save(obj=model.state_dict(), f=model_epoch)
```

> Epoch 1 128 / 50000 Loss 2.308591842651367
> Epoch 2 128 / 50000 Loss 2.3025848865509033
> Epoch 3 128 / 50000 Loss 2.3025848865509033
> Epoch 4 128 / 50000 Loss 2.3025848865509033
> Epoch 5 128 / 50000 Loss 2.3025848865509033
> ......

* 训练调参

训练时发现Loss一直不下降，参考以下调参技巧对模型和训练参数进行调整：

1. 模型结构和特征工程存在问题——修正网络结构，检查参数是否错误
2. 权重初始化方案有问题
3. 正则化过度，模型欠拟合
4. 选择合适的激活函数、损失函数
5. 选择合适的优化器和学习率
6. 梯度消失、梯度爆炸——使用Dropout、BatchNorm等方法减少学习参数
7. batch_size不合理
8. 数据集本身有问题——作无量纲化、打乱等
9. 训练时间不够长

最终发现是优化器的选择不合适，当调整为使用**Adagrad优化器**后，Loss下降速度加快：

> Epoch 1 12832 / 50000 Loss 2.197270393371582
> Epoch 1 16032 / 50000 Loss 2.1675164699554443
> Epoch 1 19232 / 50000 Loss 2.0508370399475098
> Epoch 1 22432 / 50000 Loss 1.837303638458252
> Epoch 1 25632 / 50000 Loss 1.862862467765808

### 4. 预测（可视化）

实例化Vgg-16网络模型，加载已训练好的模型参数字典，字典包含每一层的线性变换参数Weight和bias。按照网络结构依次对上一层输入进行卷积、激活等操作，将每一层的特征图（feature map）可视化输出。由于每一层卷积核数量不同（64、128、256、512），随机选取10个卷积核对应的特征图。网络前13层为卷积层，后3层全连接层没有高宽维度，最后输出各类别预测的概率值，因此仅对卷积层作可视化处理。

``` python
import torch
from torch import nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import random
import numpy as np
 
from vgg import VGG16
 
# 加载模型参数
model = VGG16()     # 由于使用GPU训练保存的模型，加载时转为CPU类型的参数
model.load_state_dict(torch.load(f='./models/cifar10_epoch_40.pth', map_location=torch.device('cpu')))
# 读取模型参数
params = list(model.named_parameters())
# 读取图片数据
im, target = test_dataset.__getitem__(0)     # TensorFloat, 32, 32, 3
print("所属分类", target)
im = im.reshape(1, 3, 32, 32)     # [1, 3, width, height]
 
# 获取每一卷积层的输出结果，传入层数和上一层输入
def get_conv_output(params, layer, conv_in):
    print(f"【第 {layer} 层】")
    # 读取第一层weight和bias进行卷积操作
    weight, bias = params[(layer - 1) * 2], params[(layer - 1) * 2 + 1]
    print("权重", weight[0], weight[1].shape)  # [64, 3, 3, 3]
    print("偏置", bias[0], bias[1].shape)  # [64]
    # 卷积
    conv_out = F.conv2d(input=conv_in, weight=weight[1], bias=bias[1], padding=1)
    print("卷积输出", conv_out.shape)  # [1, 64, 32, 32]
    conv_out = nn.ReLU()(conv_out)    # 激活
    # 保存特征图
    save_feature_map(conv_out, feature_num=10)
    # 池化
    if layer in [2, 4, 7, 10, 13]:
        conv_out = nn.MaxPool2d(kernel_size=2, stride=2)(conv_out)
 
    return conv_out
 
# 获取每一全连接层的输出结果，传入层数和上一层输入
def get_linear_output(params, layer, linear_in):
    print(f"【第 {layer} 层】")
    # 读取第一层weight和bias进行卷积操作
    weight, bias = params[(layer - 1) * 2], params[(layer - 1) * 2 + 1]
    print("权重", weight[0], "shape", weight[1].shape)  # [64, 3, 3, 3]
    print("偏置", bias[0], "shape", bias[1].shape)  # [64]
    # 全连接
    layer_out = F.linear(linear_in, weight[1], bias[1])
    print("全连接输出", layer_out.shape)
    layer_out = nn.ReLU()(layer_out)        # 激活
    # TODO 保存特征图
    # save_feature_map(layer_out, 10)
 
    return layer_out
 
# 保存特征图
def save_feature_map(conv_out, feature_num: int):
    feature_maps = conv_out.detach().numpy()  # Tensor转numpy
    # 降维，(1, 64, 32, 32) -> (64, 32, 32)
    feature_maps = feature_maps.squeeze()       # 512, 2, 2 -> 100
    # if feature_maps.shape[0] <= 100:
    #     feature_maps = np.expand_dims(np.expand_dims(feature_maps, 1), 1)
    print(feature_maps.shape)
    # 随机选取一个卷积核得到的特征图
    for i in range(feature_num):
        random_feature_map = feature_maps[random.randint(0, feature_maps.shape[0] - 1)]  # 第一维参数为特征图数量
        display_feature_maps.append( random_feature_map )
 
# 显示读入图像
plt.imshow(test_dataset.data[0])
display_feature_maps = []
 
# 1-13层     卷积层
last_conv_out = im     # 第一次卷积输入为原始图像
for layer in range(1, 14):      # 前13层依次卷积，上一次卷积结果传入下一层输入
    last_conv_out = get_conv_output(params, layer, last_conv_out)
# 14-16层    全连接层
layer13_out = nn.Flatten()(last_conv_out)     # 展平
print("【展平】", layer13_out.shape)
last_linear_out = layer13_out     # 第一次全连接输入为13层展平结果
for layer in range(14, 17):      # 前13层依次卷积，上一次卷积结果传入下一层输入
    last_linear_out = get_linear_output(params, layer, last_linear_out)
print(last_linear_out)
print("输出分类", torch.argmax(last_linear_out, axis=1).numpy()[0])
 
# 绘制每一层的特征图
plt.figure()
for idx, i in enumerate(np.arange(0, 130).reshape(13, 10).T.reshape(130) + 1):
    plt.subplot(10, 13, idx+1)
    # print(display_feature_maps[i-1].shape)
    plt.imshow(display_feature_maps[i-1], cmap='gray')
    plt.xticks([])
    plt.yticks([])
plt.show()
```

## 三、实验结果

### 1. CIFAR-10数据集

在对CIFAR-10数据集的实验过程中，经过50轮训练得到损失值Loss低于0.01，且对测试数据集的验证准确率高达99.1%。

第一张：输入Cat类别图像，输出Softmax值为：[ 0.0000,  4.6779,  2.4917, 23.1221,  0.0000, 11.7948,  8.8059,  0.0000, 9.1107,  0.0000]，其中最大值对应类别为Cat，其次为Dog。下图为各个卷积层可视化结果：

<table>
    <tr>
        <td><img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/cat.png" alt="cat" style="zoom: 25%;" /></td>
        <td><img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/cat_visual.png" alt="cat_visual" style="zoom:150%;" /></td>
        <td>
            <ul>
                <li>Cat:23.1</li>
                <li>Dog:11.7</li>
                <li>Ship:9.1</li>
                <li>Car:4.6</li>
                <li>Bird:2.4</li>
                </ul>      
        </td>
    </tr>
</table>

第二张：输入Dog类别图像，输出Softmax值为：[ 1.3165,  0.0000,  9.1405, 11.5022,  3.2681, 25.5454,  0.0000, 11.5654, 0.0000,  0.6764]，其中最大值对应类别Dog，其次为Cat。下图为各个卷积层可视化结果：

<table>
    <tr>
        <td><img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210425201444402.png" alt="image-20210425201444402" style="zoom:33%;" /></td>
        <td><img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210425201519222.png" alt="image-20210425201519222" style="zoom:80%;" /></td>
        <td>
            <ul>
                <li>Dog:25.5</li>
                <li>Cat:11.5</li>
                <li>Horse:11.5</li>
                <li>Bird:9.1</li>
                <li>Deer:3.2</li>
                </ul>      
        </td>
    </tr>
</table>

### 2. 人脸数据集

由于自收集的明星人脸数据集本身质量不高、学习参数过多等问题，使用VGG-16网络训练了500轮依然没有将损失降到较低，因此所得模型仅用于可视化卷积层，模型输出值没有参考价值。

> ......
> Epoch 500 5440 / 6734 Loss 1.2500115633010864
> Epoch 500 5760 / 6734 Loss 1.1513397693634033
> Epoch 500 6080 / 6734 Loss 1.1512929201126099
> Epoch 500 6400 / 6734 Loss 1.1513009071350098
> Epoch 500 6720 / 6734 Loss 0.8223527073860168

因为所得模型并不能拟合该分类任务，在卷积层上的直接表现就是部分卷积核不能正确提取出输入目标的特征，参数为零矩阵使得卷积结果为全0的黑色图，而且各个层之间提取的特征混乱无关联。

<table>
    <tr>
        <td><img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/0004.jpg" alt="0004" style="zoom: 50%;" /></td>
        <td><img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210425105659102.png" /></td>
    </tr>
</table>

<table>
    <tr>
        <td><img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210425203849928.png" alt="image-20210425203849928" style="zoom:30%;" /></td>
        <td><img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210425203824784.png" /></td>
    </tr>
</table>

### 3. 总结

本次实验使用Pytorch构建了VGG-16卷积神经网络模型，并分别对两个不同数据集进行了学习训练和卷积层可视化。在简单数据集上，该模型取得不错训练和预测效果；但对于较为复杂的模型，需要更多数据和训练调试技巧。另一方面，VGG模型本身学习参数量大、消耗内存资源高，增加了训练成本和难度。

在深度学习的可解释性上，本实验仅针对卷积网络每一层的卷积结果进行了可视化。卷积是图像处理的一种方式，用于提取边缘、纹理、去噪、锐化等，在CNN中，前几层提取的图像特征可以看做是普通的图像信息提取结果，池化、激活等操作进一步“压缩”这些特征信息，随着深度加深，特征“堆叠”越来越厚，可解析性也越来越差。