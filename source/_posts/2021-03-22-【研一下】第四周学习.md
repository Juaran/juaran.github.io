---

title: 【研一下】第四周学习汇报
date: 2021-3-23
category: 机器学习
tag: 周报

---



# Day 01

入门TensorFlow，学习TensorFlow2.0的基本概念。

## 1.1 运行机制

**图**：由数据和操作组成，数据的基本单位是tensor张量，操作是运算operation

**会话**：数据的运算在会话Session中完成。

原生Python运算：

``` python
a, b = 1, 2  // 定义数据
c = a + b    // 执行运算
```

TensorFlow运算：

```python
a = tf.constant(1)  // 创建常量类型的张量数据
b = tf.constant(2)    // 创建常量类型的张量数据
c = a + b    // 定义运算，c 是一个加法指令
 
with tf.Session() as s:  // 进入会话上下文环境
    s.run(c)        // 执行运算
print(c)    // 输出结果
```

模型的运行机制是：**数据、操作和运算分离**。

虽然数据Tensor和运算Flow分离，但是看起来看起来很麻烦！于是在TensorFlow2.2以后，会话机制已经隐藏起来，默认开启了eager execution**立即执行模式**，如下：

```python
a = tf.constant(1)  // 创建常量类型的张量数据
b = tf.constant(2)    // 创建常量类型的张量数据
c = tf.add(a, b)     // 理解执行add运算
```

## 1.2 Tensor类型定义

**张量**：不管是**标量、向量、矩阵**都属于张量，Tensor可以代表所有的数据类型，可以看做是不同维数的数组或列表。

类型dtype：int32、int64、float32、float64、string等

**张量的阶**（rank）：有多少层括号称为所少阶

| rank |      实例      |         例子         |
| :--: | :------------: | :------------------: |
|  0   | 标量，只有大小 |        a = 1         |
|  1   | 向量，大小方向 |    b = [1, 2, 3]     |
|  2   |  矩阵，二维表  | c = [[1, 2], [3, 4]] |
|  n   |      n阶       |                      |

**张量的形状**(shape)：即数据的形状，几行几列，(m, n) 即m行n列的矩阵。

## 1.3 Tensor操作函数

### 1.3.1 创建张量

|             函数              |                          描述                          |
| :---------------------------: | :----------------------------------------------------: |
|   tf.constant(value, shape)   | 生成常量，tf.constant(1, (2, 3)) ==> [[11 1], [1 1 1]] |
|     tf.ones(shape, dtype)     |                生成全1的指定shape的张量                |
|    tf.zeros(shape, dtype)     |                        全0张量                         |
| tf.random_uniform(shape, ...) |          生成**均匀分布**的随机数，可设定范围          |
| tf.ramdom_normal(shape, ...)  |       生成**正态分布**随机数，可设定标准差和均值       |
| tf.linspace(start, stop, num) |       生成等差数列的数值，左右均闭合，num为数量        |
|     tf.range(start, ...)      |            生成等差数列范围数值，左闭合右开            |

### 1.3.2 张量的变换

* 类型变换：转换张量的数据类型dtype

  * tf.cast(x, dtype)

* 形状变换

  * shape    返回张量形状

    ```
    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2])>
    ```

  * size    返回张量内元素数量

    ```
    <tf.Tensor: shape=(), dtype=int32, numpy=4>
    ```

  * reshape    转为特定形状

    ```
    <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[1, 2, 3, 4]])>
    ```

### 1.3.3 张量运算

因为张量的运算不仅仅是标量运算只涉及到 + - * / 等基本运算，往往还有向量、矩阵的加法、矩阵乘积等复杂运算，因此这些运算封装到了TensorFlow的运算OP函数内，以 **tf.运算函数(操作数)** 的方式进行计算。

* 标量运算
  * add    加法

  * substract    减法

  * multiply  乘法

  * divide      除法

  * exp    e的次幂

    ...

* 向量运算

* 矩阵运算

  * matmul    矩阵乘法

## 1.4 变量Variable

特点：存储持久化、可修改值、可指定被训练。可以将模型中的变量...暂时还不知道跟普通变量不同的地方。

建议使用`w = tf.Variable(tf.constant([xxx]))`的方式来创建并申明变量。

# Day 02

使用TensorFlow实现线性回归Linear Regression。

## Step 1 准备数据

样本数量100，输入train_X选取[-1, 1]之间的等差数列，输出train_Y选取为2*train_X，通过标准正态分布加入噪声浮动：

``` python
n_samples = 100
train_X = np.linspace(-1, 1, n_samples)
train_Y = 2 * train_X + np.random.randn(n_samples)*0.3
 
plt.plot(train_X, train_Y, 'ro', label='Original data')
plt.legend()
plt.show()
```

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210324090531315.png" alt="image-20210324090531315" style="zoom: 67%;" />

## Step 2 模型定义

模型函数为线性回归函数：y = W * x + b，W为权重，b为偏置

``` python
# 随机初始化权重和偏置
W = tf.Variable(np.random.randn(), name="weight")
b = tf.Variable(np.random.randn(), name="bias")
 
# 线性回归函数
def liner_regression(x):
    return tf.multiply(W, x) + b
```

## Step 3 定义损失

采用最小二乘法求预测值predict_Y和真实值train_Y之间的损失

``` python
# 损失函数
def mean_square(predict_Y, train_Y):
    return tf.reduce_sum(tf.pow(train_Y - predict_Y, 2)) / n_samples
```

## Step 4 优化损失

采用梯度下降算法优化损失函数。

``` python
# 优化器采用随机梯度下降(SGD)
learning_rate = 0.01  # 学习率
optimizer = tf.optimizers.SGD(learning_rate)
 
# 计算梯度，更新参数
def run_optimization():
    with tf.GradientTape() as tape:
        predict_Y = (train_X)
        loss = mean_square(predict_Y, train_Y)
    # 计算梯度
    gradients = tape.gradient(loss, [W, b])
    # 更新W, b
    optimizer.apply_gradients(zip(gradients, [W, b]))
 
training_steps = 1000  # 训练次数
display_step = 50  # 训练50次输出一次
# 开始训练
for step in range(1, training_steps+1):
    run_optimization()
    if step % display_step == 0:
        pred = linear_regression(X)
        loss = mean_square(pred, Y)
        print("step: %i, loss: %f, W: %f, b: %f" % (step, loss, W.numpy(), b.numpy()))
```

```
step: 50, loss: 0.520692, W: 0.819331, b: -0.107484
step: 100, loss: 0.303959, W: 1.144804, b: -0.066837
step: 150, loss: 0.195750, W: 1.376180, b: -0.052034
step: 200, loss: 0.141239, W: 1.540665, b: -0.046643
step: 250, loss: 0.113714, W: 1.657597, b: -0.044680
step: 300, loss: 0.099806, W: 1.740723, b: -0.043965
step: 350, loss: 0.092778, W: 1.799817, b: -0.043705
step: 400, loss: 0.089227, W: 1.841828, b: -0.043610
step: 450, loss: 0.087432, W: 1.871692, b: -0.043576
step: 500, loss: 0.086525, W: 1.892922, b: -0.043563
step: 550, loss: 0.086066, W: 1.908015, b: -0.043558
step: 600, loss: 0.085834, W: 1.918745, b: -0.043557
step: 650, loss: 0.085717, W: 1.926372, b: -0.043556
step: 700, loss: 0.085658, W: 1.931794, b: -0.043556
step: 750, loss: 0.085628, W: 1.935649, b: -0.043556
step: 800, loss: 0.085613, W: 1.938389, b: -0.043556
step: 850, loss: 0.085606, W: 1.940338, b: -0.043556
step: 900, loss: 0.085602, W: 1.941723, b: -0.043556
step: 950, loss: 0.085600, W: 1.942707, b: -0.043556
step: 1000, loss: 0.085599, W: 1.943407, b: -0.043556
```

画出预测的回归曲线图：

<img src="https://gitee.com/juaran/typora-image/raw/master/image-20210324094600185.png" alt="image-20210324094600185" style="zoom:67%;" />

参考：https://blog.csdn.net/wardseptember/article/details/101861034

# Day 03

* 了解Tenesorflow的不同文件读取流程，二进制文件、图像文件、文本文件都是以队列的方式先读入再进行解码操作
* 模型的加载与保存
* TFRecords保存样本特征的二进制文件

# Day 04 Mnist手写数字识别

## 1. MNIST数据集

### 1.1 数据集导入

在TensorFlow2.x中，一些数据公共数据集通过内置keras导入，导入方式为：

``` python
from tensorflow.keras.datasets import mnist
 
(train_x,train_y),(test_x,test_y) = mnist.load_data()
train_x.shape, train_y.shape, test_x.shape, test_y.shape
```

> ```
> ((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))
> ```

数据集均以ndarray类型、格式为uin8存储。分别是：

* 训练样本60000
  * 输入为28*28的图片
  * 输出为数字0-9的标量
* 测试样本10000

查看第一个训练样本输入和输出：

``` python
import matplotlib.pyplot as plt
 
plt.imshow(train_x[0], cmap="Greys")
print(train_y[0])     # 5
```

<img src="https://gitee.com/juaran/typora-image/raw/master/image-20210327161122642.png" alt="image-20210327161122642" style="zoom:50%;" />

### 1.2 数据预处理

本次手写数字识别的分类任务仅使用单个神经元——Softmax算法实现。

* 对每一个输入数据为[28, 28]的二维数组图像，首先作归一化处理，然后reshape成[784, ]的一维数组图像，即每个手写数字图像可看作[x_1, ...., x_784]的输入层
* 对于0-9的输出，将其进行one-hot编码，即:
  * 0-10000000000
  * 1-01000000000
  * 2-00100000000
  * ......
  * 9-00000000001

* 数据类型由ndarray-uint8转为Tensor-float32和Tensor-int32

<img src="https://gitee.com/juaran/typora-image/raw/master/20210306225842624.png" alt="在这里插入图片描述" style="zoom:50%;" />

（借用CSDN博客图，中间隐含层去掉）

``` python
# 数据预处理
 
X_train, X_test = tf.cast(train_x/255.0, tf.float32), tf.cast(test_x/255.0, tf.float32)        # 类型转换，归一化
X_train, X_test = tf.reshape(X_train, [-1, 784]), tf.reshape(X_test, [-1, 784])            # 维度转换
 
Y_train, Y_test = tf.cast(train_y, tf.int32), tf.cast(test_y, tf.int32)
# One-hot编码表示
Y_train, Y_test = tf.one_hot(Y_train, 10, 1, 0), tf.one_hot(Y_test, 10, 1, 0)
```

> X_train.shape:
> TensorShape([60000, 784])
> Y_train.shape:
> TensorShape([60000, 10])

## 2. 模型构造

### 2.1 参数定义

* W：权重，维度是[784, 10]，X_train[1, 784] * W[784, 10] = [1, 10]。标准正态分布随机值
* b：偏置，维度[10]，标准正态分布随机值
* y_predict：预测输出

``` python
# 模型参数
W = tf.Variable(initial_value=tf.random.normal(shape=[784, 10]))
b = tf.Variable(initial_value=tf.random.normal(shape=[10]))
y_predict = tf.matmul(X_train, W) + b
```

> W：
> <tf.Variable 'Variable:0' shape=(784, 10) dtype=float32, numpy=
> b:
> <tf.Variable 'Variable:0' shape=(10,) dtype=float32, numpy=
> y_predict:
> <tf.Tensor: shape=(60000, 10), dtype=float32, numpy=

### 2.2 损失函数

* 使用softmax算法在线性模型`y_predict = tf.matmul(X_train, W) + b`中加入非线性因素，以解决线性模型表达能力不足的缺陷，能够处理分类问题，也称为激活函数
* 使用交叉熵作为该分类问题的损失函数计算方法，交叉熵越小，代表模型越准确

``` python
# 交叉熵损失函数
def loss_function():
    y_predict = tf.matmul(X_train, W) + b
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
        labels=Y_train, logits=y_predict))
       return loss
```

> loss:
>
> <tf.Tensor: shape=(), dtype=float32, numpy=11.994308>

### 2.3 模型优化

采用一般的梯度下降算法gradient descent优化损失，分别对W, b计算梯度，并更新参数：

``` python
# 梯度下降优化损失
learning_rate = 0.1    # 学习率
optimizer = tf.optimizers.SGD(learning_rate)  # 优化器
 
def run_optimizer():
    with tf.GradientTape() as tape:
        loss = loss_function()
 
    gradients = tape.gradient(loss, [W, b])
    optimizer.apply_gradients(zip(gradients, [W, b]))
```

## 3. 训练

不断执行梯度下降优化器，更新参数W和b，使每一次loss降低。

``` python
# 迭代训练
training_epochs = 1000
display_step = 100
 
for epcho in range(training_epochs):
    run_optimizer()
    if epcho % display_step == 0:
        print("Epoch", epcho, ", Loss =", loss.numpy())
```

> Epoch 0 , Loss = 0.7560682
> Epoch 100 , Loss = 0.74077475
> Epoch 200 , Loss = 0.72669786
> Epoch 300 , Loss = 0.7136781
> Epoch 400 , Loss = 0.7015847
> Epoch 500 , Loss = 0.6903085
> Epoch 600 , Loss = 0.6797586
> Epoch 700 , Loss = 0.66985756
> Epoch 800 , Loss = 0.66053957
> Epoch 900 , Loss = 0.65174806

### 4. 测试

使用训练好的模型对测试数据进行预测并与真实值对比，计算模型的精确度：

```python
# 模型评估
y_predict = tf.matmul(X_test, W) + b  # 计算测试数据的预测值
 
# 评估列表，精确度计算
equal_list = tf.equal(tf.argmax(y_predict, 1), tf.argmax(Y_test, 1))
accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32)).numpy()
equal_list, accuracy
```

> equal_list：
> <tf.Tensor: shape=(10000,), dtype=bool, numpy=array([ True,  True,  True, ...,  True, False,  True])>
>
> accuracy：
>
> 0.852

找一张图片输出：

``` python
img = tf.reshape(X_test[1], shape=[28, 28])  # 维度转换
plt.imshow(img, cmap='Greys')
print(tf.argmax(y_predict[1]).numpy())    # 预测输出：2
```

<img src="https://gitee.com/juaran/typora-image/raw/master/image-20210327172925575.png" alt="image-20210327172925575" style="zoom:50%;" />

 

参考：

【神经网络与深度学习】使用MNIST数据集训练手写数字识别模型——[附完整训练代码] https://blog.csdn.net/weixin_45954454/article/details/114455209

TensorFlow MNIST 数据集 https://blog.csdn.net/weixin_46274168/article/details/114109017

# Day 05

**感知机**是根据输入实例的特征向量x对其进行二分类的**线性分类模型**：
$$
f(x)=sign(w⋅x+b)
$$

$$
sign(x) = \left\{
  \begin{array}{lr}
    +1 & : x \ge 0\\
    -1 & : x < 0 \\
  \end{array}
\right.
$$

<img src="https://img.cntofu.com/book/MachineLearning//assets/感知机模型.jpg" alt="img" style="zoom:50%;" />

 

感知机的激活函数是sign（符号函数），**逻辑回归**的激活函数是sigmod：
$$
sigmod(x)=\frac{e^x}{1+e^x}=\frac{1}{1+e^{−x}}
$$
经过sigmod函数的输出会落在（0，1）区间内，能把输入“压缩”成0~1之间。在二分类问题中，超过0.5即可认为发生，低于0.5则发生的概率小。

<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/4550574-8c0de2e3811db671.png" />

**激活函数**的目的是在线性模型中加入非线性因素，以解决线性模型的表达能力不足的缺陷，即能够表达出更多的假设空间函数。常见的激活函数有Sigmoid、Tanh和ReLU等。 

**Softmax**函数用于处理**多分类**问题，可以看做逻辑分类回归分类问题的一般化。Softmax计算每个类别发生的概率，最终取最大值所属类别作为最终分类。
$$
S_i \ = \frac {e^i} {\sum_j e^j}
$$
<img src="https://cdn.jsdelivr.net/gh/juaran/juaran.github.io@image/typora/image-20210328112913314.png" alt="image-20210328112913314" style="zoom: 67%;" />

通常使用**交叉熵损失**函数来作为多分类问题的损失函数。其中y*为真实值，y为预测值。即计算真实分布和预测分布之间的“差距大小”。交叉熵越小代表预测结果越准确。
$$
H_{y^*}(y) = - \sum y_i^*\ log \ y_i
$$

 